# Q-LEARNING VISUAL GUIDE
## Diagrams and Flowcharts

---

## ğŸ“Š Q-TABLE STRUCTURE

```
Q-Table: [States Ã— Actions]

                    ACTIONS
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Scale Down â”‚ Do Nothing â”‚ Scale Up â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ State 0 â”‚    0.5     â”‚    1.2     â”‚   -0.3   â”‚  â† Low CPU, Low Traffic, 3 servers
    â”‚ State 1 â”‚   -0.8     â”‚    0.9     â”‚    1.5   â”‚  â† Med CPU, Low Traffic, 3 servers
S   â”‚ State 2 â”‚    1.1     â”‚    0.4     â”‚    0.7   â”‚  â† High CPU, Low Traffic, 3 servers
T   â”‚ State 3 â”‚    0.3     â”‚    1.8     â”‚    0.2   â”‚  â† Low CPU, Med Traffic, 3 servers
A   â”‚   ...   â”‚    ...     â”‚    ...     â”‚    ...   â”‚
T   â”‚ State 87â”‚   -0.2     â”‚    2.1     â”‚    1.3   â”‚
E   â”‚ State 88â”‚    0.9     â”‚    1.5     â”‚    0.8   â”‚
S   â”‚ State 89â”‚    1.4     â”‚    0.6     â”‚   -0.5   â”‚  â† High CPU, High Traffic, 10 servers
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Higher Q-value = Better action for that state
Agent chooses action with highest Q-value (exploitation)
```

---

## ğŸ”„ Q-LEARNING UPDATE FLOW

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Q-LEARNING ALGORITHM                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: OBSERVE STATE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   State (s)  â”‚  Example: CPU=60%, Traffic=High, Servers=5
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
Step 2: CHOOSE ACTION (Îµ-greedy)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  if random() < Îµ:                    â”‚
â”‚      action = random()  â† EXPLORE    â”‚
â”‚  else:                               â”‚
â”‚      action = argmax(Q[s]) â† EXPLOIT â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
Step 3: TAKE ACTION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Action (a)  â”‚  Example: Scale Up
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
Step 4: OBSERVE OUTCOME
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Reward (r)        = +1.5          â”‚
â”‚  Next State (s')   = CPU=45%, ...  â”‚
â”‚  Done              = False         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
Step 5: UPDATE Q-TABLE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  current_q = Q[s, a]                                       â”‚
â”‚                                                            â”‚
â”‚  if done:                                                  â”‚
â”‚      td_target = r                                         â”‚
â”‚  else:                                                     â”‚
â”‚      td_target = r + Î³ Ã— max(Q[s'])                        â”‚
â”‚                                                            â”‚
â”‚  td_error = td_target - current_q                          â”‚
â”‚                                                            â”‚
â”‚  Q[s, a] = current_q + Î± Ã— td_error  â† UPDATE!             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€â”€â”€â”€â”€â”€â–º REPEAT (until episode ends)
```

---

## ğŸ“ Q-LEARNING UPDATE FORMULA BREAKDOWN

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚  Q(s,a) â† Q(s,a) + Î± [r + Î³ max Q(s',a') - Q(s,a)]                 â”‚
â”‚  â””â”€â”¬â”€â”˜   â””â”€â”¬â”€â”˜   â”‚ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”¬â”€â”˜                   â”‚
â”‚    â”‚       â”‚     â”‚ â”‚   â”‚       â”‚             â”‚                     â”‚
â”‚    â”‚       â”‚     â”‚ â”‚   â”‚       â”‚             â””â”€ Current estimate   â”‚
â”‚    â”‚       â”‚     â”‚ â”‚   â”‚       â””â”€ Best future value                â”‚
â”‚    â”‚       â”‚     â”‚ â”‚   â””â”€ Discount factor (0-1)                    â”‚
â”‚    â”‚       â”‚     â”‚ â””â”€ Immediate reward                             â”‚
â”‚    â”‚       â”‚     â””â”€ Learning rate (0-1)                            â”‚
â”‚    â”‚       â””â”€ Old Q-value                                          â”‚
â”‚    â””â”€ New Q-value                                                  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INTUITION:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
New estimate = Old estimate + Learning_rate Ã— (Target - Old estimate)
                               â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                   TD Error
                                (How wrong we were)
```

---

## ğŸ¯ EPSILON-GREEDY POLICY

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Îµ-GREEDY DECISION TREE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    Generate random number
                           (0 to 1)
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  random() < Îµ ?     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚                           â”‚
               YES                         NO
                â”‚                           â”‚
                â–¼                           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   EXPLORE     â”‚         â”‚    EXPLOIT      â”‚
        â”‚               â”‚         â”‚                 â”‚
        â”‚ Random action â”‚         â”‚ Best Q-value    â”‚
        â”‚ from all      â”‚         â”‚ action          â”‚
        â”‚ actions       â”‚         â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                           â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                        Take action


EPSILON DECAY OVER TIME:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Îµ = 1.0 â”¤ â—â—â—â—â—â—â—â—
        â”‚         â—â—â—â—â—
        â”‚             â—â—â—â—â—
        â”‚                 â—â—â—â—â—
Îµ = 0.5 â”¤                     â—â—â—â—â—
        â”‚                         â—â—â—â—â—
        â”‚                             â—â—â—â—â—
        â”‚                                 â—â—â—â—â—
Îµ = 0.05â”¤                                     â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        0        100       200       300       400       500
                        Episode Number

Early: Explore (Îµ â‰ˆ 1.0)  â†’  Later: Exploit (Îµ â‰ˆ 0.05)
```

---

## ğŸŒŠ TEMPORAL DIFFERENCE (TD) LEARNING

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TD ERROR CALCULATION                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Current Q-value:  Q(s,a) = 0.5
                           â”‚
                           â”‚  We take action 'a' in state 's'
                           â”‚
                           â–¼
Actual experience:  r = +1.0  (immediate reward)
                    s' = next state
                    max Q(s') = 1.2  (best future value)
                           â”‚
                           â”‚  Calculate TD Target
                           â–¼
TD Target = r + Î³ Ã— max Q(s')
          = 1.0 + 0.95 Ã— 1.2
          = 1.0 + 1.14
          = 2.14
                           â”‚
                           â”‚  Calculate TD Error
                           â–¼
TD Error = TD Target - Current Q
         = 2.14 - 0.5
         = +1.64  â† We underestimated! Increase Q-value
                           â”‚
                           â”‚  Update Q-value
                           â–¼
New Q = Old Q + Î± Ã— TD Error
      = 0.5 + 0.1 Ã— 1.64
      = 0.5 + 0.164
      = 0.664  â† Updated Q-value!


IF TD ERROR IS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Positive (+) â†’ We got MORE than expected â†’ INCREASE Q-value
  Negative (-) â†’ We got LESS than expected â†’ DECREASE Q-value
  Zero (0)     â†’ Perfect prediction â†’ NO CHANGE
```

---

## ğŸ—ï¸ CLOUD AUTO-SCALING STATE SPACE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STATE REPRESENTATION                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

State = (CPU_level, Traffic_level, Server_count)
         â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚            â”‚               â”‚
             â”‚            â”‚               â””â”€ 10 values (1-10 servers)
             â”‚            â””â”€ 3 values (Low, Med, High)
             â””â”€ 3 values (Low, Med, High)

Total States = 3 Ã— 3 Ã— 10 = 90 states


EXAMPLE STATES:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

State 0:  (Low CPU,  Low Traffic,  1 server)   â†’ Probably over-provisioned
State 15: (Med CPU,  Med Traffic,  3 servers)  â†’ Good balance!
State 44: (High CPU, High Traffic, 5 servers)  â†’ Need to scale up!
State 89: (High CPU, High Traffic, 10 servers) â†’ At max capacity!


STATE TRANSITIONS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Current State: (Med CPU, Med Traffic, 3 servers)
                              â”‚
                              â”‚ Action: Scale Up
                              â–¼
Next State:    (Low CPU, Med Traffic, 4 servers)
                              â”‚
                              â”‚ Reward: +1.5 (good action!)
                              â–¼
                        Update Q-table
```

---

## ğŸ® COMPLETE TRAINING LOOP

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRAINING ALGORITHM                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

FOR episode = 1 to 500:
    â”‚
    â”œâ”€â–º agent.begin_episode()  â† Increment counter, decay Îµ
    â”‚
    â”œâ”€â–º state = env.reset()    â† Start new episode
    â”‚
    â”œâ”€â–º FOR step = 1 to 200:
    â”‚       â”‚
    â”‚       â”œâ”€â–º action = agent.choose_action(state)  â† Îµ-greedy
    â”‚       â”‚
    â”‚       â”œâ”€â–º next_state, reward, done = env.step(action)
    â”‚       â”‚
    â”‚       â”œâ”€â–º agent.update(state, action, reward, next_state, done)
    â”‚       â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚       â”‚                          Q-learning update
    â”‚       â”‚
    â”‚       â”œâ”€â–º state = next_state
    â”‚       â”‚
    â”‚       â””â”€â–º IF done: BREAK
    â”‚
    â””â”€â–º END FOR


CONVERGENCE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Episode 1-100:   Random exploration, learning basics
Episode 100-300: Refining policy, reducing exploration
Episode 300-500: Fine-tuning, mostly exploitation
Episode 500+:    Converged to near-optimal policy
```

---

## ğŸ“ˆ REWARD STRUCTURE FOR CLOUD AUTO-SCALING

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      REWARD FUNCTION                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Reward = Performance_reward + SLA_penalty + Cost_penalty

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PERFORMANCE REWARD                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CPU in 40-70% (optimal):    +2.0     â”‚  â† Good performance
â”‚ CPU outside range:          -1.0     â”‚  â† Suboptimal
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SLA PENALTY                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CPU > 80% (violation):      -3.0     â”‚  â† Bad! Users affected
â”‚ CPU â‰¤ 80%:                   0.0     â”‚  â† OK
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COST PENALTY                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Per server:                 -0.2     â”‚  â† More servers = more cost
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


EXAMPLE SCENARIOS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Scenario 1: 5 servers, CPU = 55%
  Performance: +2.0  (in optimal range)
  SLA:          0.0  (no violation)
  Cost:        -1.0  (5 Ã— 0.2)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:       +1.0  âœ“ Good!

Scenario 2: 2 servers, CPU = 85%
  Performance: -1.0  (outside range)
  SLA:         -3.0  (violation!)
  Cost:        -0.4  (2 Ã— 0.2)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:       -4.4  âœ— Bad! Scale up!

Scenario 3: 10 servers, CPU = 20%
  Performance: -1.0  (outside range)
  SLA:          0.0  (no violation)
  Cost:        -2.0  (10 Ã— 0.2)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:       -3.0  âœ— Bad! Scale down!
```

---

## ğŸ¯ KEY INSIGHTS

### 1. Q-Learning is Bootstrapping
```
Q(s,a) uses estimate of Q(s',a') to update itself
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           Bootstrapping!
```

### 2. Off-Policy Learning
```
Q-learning learns optimal policy even while following Îµ-greedy
(Can learn from suboptimal actions)
```

### 3. Value Propagation
```
Episode 1:   Only terminal state has non-zero Q-values
Episode 10:  Values propagate 1-2 steps back
Episode 100: Values propagate throughout episode
Episode 500: Optimal values everywhere
```

### 4. Exploration-Exploitation Trade-off
```
Too much exploration  â†’ Wastes time on bad actions
Too much exploitation â†’ Gets stuck in local optimum
Îµ-greedy with decay   â†’ Best of both worlds!
```

---

**Created by**: Antigravity AI  
**Purpose**: Interview preparation and understanding  
**Status**: Complete âœ“
